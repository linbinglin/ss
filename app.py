import streamlit as st
from openai import OpenAI
import re

# 1. é¡µé¢é…ç½®
st.set_page_config(page_title="AIåˆ†é•œå¯¼æ¼” v6.6", layout="wide", page_icon="ğŸ¬")

# æ•°æ®æŒä¹…åŒ–
if 'all_segments' not in st.session_state:
    st.session_state['all_segments'] = []
if 'batch_result' not in st.session_state:
    st.session_state['batch_result'] = ""

# 2. ä¾§è¾¹æ ï¼šé…ç½®ä¸­å¿ƒ
st.sidebar.title("âš™ï¸ ç³»ç»Ÿé…ç½®")
api_key = st.sidebar.text_input("1. API Key", type="password")
base_url = st.sidebar.text_input("2. æ¥å£åœ°å€", value="https://blog.tuiwen.xyz/v1")

st.sidebar.markdown("---")
st.sidebar.subheader("3. Midjourney ç”»é£åç¼€")
mj_suffix = st.sidebar.text_input("åç¼€è¯ (å¦‚: --ar 16:9 --v 6.1)", value="--ar 16:9 --v 6.1 --style raw")

st.sidebar.markdown("---")
st.sidebar.subheader("4. æ¨¡å‹è®¾ç½®")
model_options = ["gpt-4o", "claude-3-5-sonnet-20240620", "deepseek-chat", "è‡ªå®šä¹‰æ¨¡å‹"]
selected_option = st.sidebar.selectbox("é€‰æ‹©æ¨¡å‹", model_options)
if selected_option == "è‡ªå®šä¹‰æ¨¡å‹":
    model_id = st.sidebar.text_input("æ‰‹åŠ¨è¾“å…¥ Model ID")
else:
    model_id = selected_option

st.title("ğŸ¬ ç”µå½±è§£è¯´å…¨æµç¨‹åˆ†é•œå·¥å…·")
st.caption("ç¬¬ä¸€æ­¥ï¼šè¿˜åŸ V5.1 ç‰©ç†åˆ‡åˆ†é€»è¾‘ | ç¬¬äºŒæ­¥ï¼šè§†è§‰é”šå®š+æ–‡æ¡ˆå¯¹ç…§+æ‰¹å¤„ç†")

# --- ç¬¬ä¸€é˜¶æ®µï¼šè¿˜åŸ V5.1 ç‰©ç†åˆ‡åˆ† ---
st.header("ç¬¬ä¸€æ­¥ï¼šé€»è¾‘åˆ†é•œï¼ˆè¿˜åŸ V5.1 çº¯å‡€éª¨æ¶ï¼‰")

uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡ˆ (TXT)", type=['txt'])

if uploaded_file:
    raw_text = uploaded_file.getvalue().decode("utf-8", errors="ignore")
    # V5.1 æ ¸å¿ƒé€»è¾‘ï¼šæŠ¹é™¤æ ¼å¼
    clean_stream = "".join(raw_text.split())

    if st.button("ğŸš€ ç”Ÿæˆç‰©ç†åˆ†é•œè„šæœ¬", use_container_width=True):
        if not api_key or not model_id:
            st.error("è¯·é…ç½® API ä¿¡æ¯")
        else:
            client = OpenAI(api_key=api_key, base_url=base_url)
            
            # V5.1 æ ¸å¿ƒæç¤ºè¯ï¼šçº¯å‡€åˆ†é•œï¼Œä¸æ”¹å­—
            STEP1_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæå…¶ä¸¥è°¨çš„ç”µå½±åˆ†é•œå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æä¾›çš„ã€çº¯æ–‡å­—æµã€‘æ‹†è§£ä¸ºã€æ•°å­—ç¼–å·çš„åˆ†é•œè„šæœ¬ã€‘ã€‚

### æ ¸å¿ƒè§„åˆ™ï¼š
1. **ä¸¥æ ¼ç¼–å·**ï¼šæ¯ä¸€ä¸ªåˆ†é•œå¿…é¡»ä»¥â€œ1.â€ â€œ2.â€ â€œ3.â€ è¿™ç§æ•°å­—åºå·å¼€å¤´ï¼Œä¸å¾—é—æ¼ã€‚
2. **æ‹’ç»æ³¨é‡Š**ï¼šä¸¥ç¦åœ¨åˆ†é•œä¸­æ·»åŠ ä»»ä½•æ‹¬å·ã€åˆ†æã€é•œå¤´æ„å›¾æˆ–é¢å¤–æè¿°ã€‚åªéœ€è¾“å‡ºâ€œæ•°å­—. åŸæ–‡â€ã€‚
3. **è§†è§‰å•å…ƒåˆ‡åˆ†ï¼ˆä¸¥ç¦åˆå¹¶ï¼‰**ï¼š
   - ä¸€ä¸ªåˆ†é•œåªèƒ½åŒ…å«ä¸€ä¸ªè§†è§‰é‡ç‚¹æˆ–æ ¸å¿ƒåŠ¨ä½œã€‚
   - ä¸¥ç¦ä¸ºäº†çœäº‹å°†ä¸¤ä¸ªä¸åŒçš„åŠ¨ä½œåˆå¹¶åœ¨ä¸€ä¸ªåˆ†é•œé‡Œã€‚
4. **å­—æ•°ä¸æ—¶é•¿å¯¹é½**ï¼š
   - æ¯ä¸ªåˆ†é•œç›®æ ‡å­—æ•°ä¸º 25-35 å­—ã€‚
   - ç»å¯¹ä¸¥ç¦è¶…è¿‡ 40 å­—ã€‚
5. **åŸæ–‡é›¶æ”¹åŠ¨**ï¼šä¸å‡†æ”¹å­—ã€åˆ å­—ã€åŠ å­—ã€‚
"""
            with st.spinner("æ­£åœ¨ä»¥ V5.1 æœºæ¢°é€»è¾‘åˆ‡åˆ†åˆ†é•œ..."):
                try:
                    response = client.chat.completions.create(
                        model=model_id,
                        messages=[{"role": "system", "content": STEP1_PROMPT},
                                  {"role": "user", "content": f"è¯·å¯¹ä»¥ä¸‹æ–‡å­—æµè¿›è¡Œç­‰æƒé‡çš„ç²¾ç¡®åˆ†é•œï¼Œå¿…é¡»å¸¦æ•°å­—ç¼–å·ï¼š\n\n{clean_stream}"}],
                        temperature=0.1
                    )
                    st.session_state['all_segments'] = [l.strip() for l in response.choices[0].message.content.split('\n') if re.match(r'^\d+', l.strip())]
                except Exception as e:
                    st.error(f"å¤„ç†å¤±è´¥: {str(e)}")

# ç¬¬ä¸€æ­¥ç»“æœé¢„è§ˆä¸ç»Ÿè®¡
if st.session_state['all_segments']:
    st.subheader(f"ğŸ“‹ åˆ†é•œéª¨æ¶é¢„è§ˆ (å…± {len(st.session_state['all_segments'])} ç»„)")
    
    col_a, col_b = st.columns([1, 2])
    with col_a:
        for line in st.session_state['all_segments'][:10]: 
            text_only = re.sub(r'^\d+[\.ã€\s]+', '', line)
            count = len(text_only)
            if count > 40: st.error(f"âŒ {line} ({count}å­—)")
            else: st.success(f"âœ… {line} ({count}å­—)")
    
    with col_b:
        edited_text = st.text_area("âœï¸ ç‰©ç†åˆ†é•œç¼–è¾‘åŒº", "\n".join(st.session_state['all_segments']), height=300)
        st.session_state['all_segments'] = [l.strip() for l in edited_text.split('\n') if re.match(r'^\d+', l.strip())]

    st.markdown("---")

    # --- ç¬¬äºŒé˜¶æ®µï¼šåˆ†é•œæè¿° (å¯¹ç…§ç‰ˆ) ---
    st.header("ç¬¬äºŒæ­¥ï¼šåˆ†é•œè§†è§‰æè¿°ï¼ˆå¸¦æ–‡æ¡ˆå¯¹ç…§ï¼‰")
    
    c1, c2 = st.columns([1, 1])
    with c1:
        char_desc = st.text_area("ğŸ‘¤ è§’è‰²å½¢è±¡åŠç€è£…è®¾å®š (å¿…å¡«)", 
                                 placeholder="ä¾‹ï¼šæ—å‡¡ï¼š(25å²ï¼Œç„è‰²åˆºç»£é•¿è¢ï¼Œç›®å…‰å¦‚ç”µï¼Œé»‘è‰²é©¬å°¾)ã€‚\nè‹æ™´ï¼š(18å²ï¼Œç´«è‰²çº±è£™ï¼Œè´è¶ç°ª)ã€‚",
                                 height=200)
    with c2:
        total = len(st.session_state['all_segments'])
        batch_size = 20
        batch_options = [f"ç¬¬ {i+1} - {min(i+batch_size, total)} ç»„" for i in range(0, total, batch_size)]
        selected_batch = st.selectbox("é€‰æ‹©å½“å‰å¤„ç†æ‰¹æ¬¡ (æ¯æ¬¡20ç»„)", batch_options)
        
        # ç´¢å¼•æå–
        nums = re.findall(r'\d+', selected_batch)
        start_idx, end_idx = int(nums[0]) - 1, int(nums[1])

    if st.button(f"ğŸ¨ ä¸º {selected_batch} ç”Ÿæˆæè¿°è¯", use_container_width=True):
        if not char_desc:
            st.error("è¯·å¡«å†™è§’è‰²è®¾å®šï¼")
        else:
            client = OpenAI(api_key=api_key, base_url=base_url)
            current_batch_txt = "\n".join(st.session_state['all_segments'][start_idx:end_idx])
            
            # ã€é‡è¦å¾®è°ƒã€‘åœ¨è¾“å‡ºæ ¼å¼ä¸­å¼ºåˆ¶è¦æ±‚å¤è¯»æ–‡æ¡ˆ
            STEP2_PROMPT = f"""ä½ æ˜¯ä¸€ä¸ªè§†è§‰ç¾æœ¯å¯¼æ¼”ã€‚ä¸ºåˆ†é•œè„šæœ¬é…ä¸Šè§†è§‰æŒ‡ä»¤ã€‚

### æ ¸å¿ƒè§„åˆ™ï¼š
1. **æ–‡æ¡ˆå¤è¯»ï¼ˆé“å¾‹ï¼‰**ï¼šè¾“å‡ºçš„æ¯ä¸€ç»„å¿…é¡»ä»¥è„šæœ¬ä¸­çš„ã€åºå·å’Œæ–‡æ¡ˆåŸæ–‡ã€‘å¼€å¤´ï¼Œä¸¥ç¦é—æ¼æ–‡æ¡ˆã€‚
2. **è§†è§‰é”šå®š**ï¼šå½“æ–‡æ¡ˆä¸­å‡ºç°è§’è‰²æ—¶ï¼Œå¿…é¡»å®Œæ•´è°ƒç”¨ä»¥ä¸‹å½¢è±¡ï¼Œå¹¶ä½¿ç”¨æ‹¬å·åŒ…è£¹ã€‚
è§’è‰²åˆ—è¡¨ï¼š{char_desc}
3. **åœºæ™¯é”å®š**ï¼šæ¯ä¸ªåˆ†é•œå¿…é¡»æè¿°åœºæ™¯åœ°ç‚¹å’Œç¯å¢ƒæ°›å›´ï¼Œé˜²æ­¢è·³æˆã€‚
4. **æ ¼å¼åŒ–è¾“å‡ºè¦æ±‚**ï¼š
   - ç”»é¢æè¿°ï¼š[åœºæ™¯ä½ç½®], [å…‰å½±ç»†èŠ‚], (äººç‰©å®Œæ•´å½¢è±¡æå†™), [è§†è§’æ„å›¾] {mj_suffix}
   - è§†é¢‘ç”Ÿæˆï¼š[äººç‰©å…·ä½“åŠ¨ä½œ], [ç¥æ€æ¼”å˜], [é•œå¤´è¿åŠ¨]ã€‚ç¡®ä¿5ç§’å†…å®Œæˆã€‚

### è¾“å‡ºæ ¼å¼èŒƒä¾‹ï¼š
1. 8å²é‚£å¹´å®¶é‡Œç©·å¾—æ­ä¸å¼€é”…äº†
ç”»é¢æè¿°ï¼šç ´è´¥çš„å†œèˆèƒŒæ™¯, é˜´æš—çš„å…‰çº¿, (æ—å‡¡ï¼Œ8å²æ¨¡æ ·ï¼Œè¡£è¡«è¤´è¤›ï¼Œé¢é»„è‚Œç˜¦), è¿œæ™¯è§†è§’ {mj_suffix}
è§†é¢‘ç”Ÿæˆï¼šæ—å‡¡ç»æœ›åœ°çœ‹ç€ç©ºç¢—ï¼Œçœ¼çœ¶æ¹¿æ¶¦ï¼Œé•œå¤´ç¼“ç¼“æ‹‰è¿‘ã€‚
---
"""
            with st.spinner(f"æ­£åœ¨ç”Ÿæˆ {selected_batch} çš„å›¾æ–‡å¯¹ç…§æ–¹æ¡ˆ..."):
                try:
                    response = client.chat.completions.create(
                        model=model_id,
                        messages=[{"role": "system", "content": STEP2_PROMPT},
                                  {"role": "user", "content": current_batch_txt}],
                        temperature=0.4
                    )
                    st.session_state['batch_result'] = response.choices[0].message.content
                except Exception as e:
                    st.error(f"å¤„ç†å¤±è´¥: {str(e)}")

    if st.session_state['batch_result']:
        st.subheader(f"ğŸ¥ {selected_batch} è§†è§‰æ–¹æ¡ˆï¼ˆæ–‡æ¡ˆå¯¹ç…§é¢„è§ˆï¼‰")
        st.text_area("ç»“æœé¢„è§ˆï¼ˆå«æ–‡æ¡ˆåŸæ–‡ï¼‰", st.session_state['batch_result'], height=500)
        st.download_button(f"ğŸ“¥ ä¸‹è½½ {selected_batch}", st.session_state['batch_result'], file_name=f"åˆ†é•œå¯¹ç…§æè¿°_{selected_batch}.txt")

st.markdown("---")
st.caption("v6.6 | ä¿æŒ V5.1 åˆ†é•œéª¨æ¶ | å¢å¼º Step2 æ–‡æ¡ˆå¯¹ç…§ | è§’è‰²æ‹¬å·é”šå®š | ç”»é£åç¼€è‡ªåŠ¨æ‹¼æ¥")
